{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ac75a8-71c6-4d85-8a95-0bea4d70770b",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Data has been taken from [VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) dataset.\n",
    "\n",
    "The twenty object classes that have been selected are:\n",
    "\n",
    "* Person: person\n",
    "* Animal: bird, cat, cow, dog, horse, sheep\n",
    "* Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train\n",
    "* Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e166a6a-8cd9-431b-9be5-1b78fb665fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import xml.etree.ElementTree as ET\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a1d8a3-7896-4fdf-bc7a-19d3e6af3a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=[], dest='dir', nargs=None, const=None, default='..', type=None, choices=None, help='Annotations.', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Build Annotations.')\n",
    "parser.add_argument('dir', default='..', help='Annotations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24b5c04-8f71-438f-904a-8152f50bd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [('2007', 'train'), ('2007', 'val'), ('2007', 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dcb6e33-c761-460a-8a7d-e5cd34125fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_num = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n",
    "               'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n",
    "               'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n",
    "               'sofa': 17, 'train': 18, 'tvmonitor': 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c60cc635-016f-462f-9ce3-09a3da738831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_annotation(in_file):\n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    res = ''\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        class_name = obj.find('name').text\n",
    "        cls_id = classes_num.get(class_name)\n",
    "        if cls_id == None : \n",
    "            continue\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text),\n",
    "             int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n",
    "        res += ' ' + ','.join([str(a) for a in b]) + ',' + str(cls_id)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "913a6a41-2640-406f-960f-c996508e0e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_num.get('a') == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83226552-6c8d-4b7e-8e63-17b31bb45fe4",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "### Shapes and probabilities\n",
    "The input is a (448, 448, 3) image and the output is a (7, 7, 30) tensor. The output is based on S x S x (B * 5 +C).\n",
    "\n",
    "B = 2 in YOLO-v1 and no of classes = 20 => 2 * 5 + 20 = 30.\n",
    "\n",
    "S X S is the number of grids, B is the number of bounding boxes per grid, C is the number of predictions per grid\n",
    "\n",
    "The system divides the image into an S x S grid. Each of these grid cells predicts B bounding boxes and confidence scores for these boxes. The confidence score indicates how sure the model is that the box contains an object and also how accurate it thinks the box is that predicts. The confidence score can be calculated using the formula:\n",
    "$C = Pr(object) * IoU$\n",
    "\n",
    "$IoU$: Intersection over Union between the predicted box and the ground truth.\n",
    "\n",
    "If no object exists in a cell, its confidence score should be zero.\n",
    "\n",
    "<img src =\"images/bbox.png\" width= \"250px\"/>\n",
    "\n",
    "Each grid cell also predicts C conditional class probabilities $Pr(Classi|Object)$.\n",
    "\n",
    "It only predicts one set of class probabilities per grid cell, regardless of the number of boxes B. During testing, these conditional class probabilities are multiplied by individual box confidence predictions which give class-specific confidence scores for each box. These scores show both the probability of that class and how well the box fits the object.\n",
    "\n",
    "$Pr(Class i|Object)*Pr(Object)*IoU = Pr(Class i)*IoU$.\n",
    "\n",
    "The final predictions are encoded as an S x S x (B*5 + C) tensor.\n",
    "\n",
    "### Intersection Over Union (IoU):\n",
    "IoU is used to evaluate the object detection algorithm. It is the overlap between the ground truth and the predicted bounding box, i.e it calculates how similar the predicted box is with respect to the ground truth.\n",
    "\n",
    "![image.png](images/iou.png)\n",
    "\n",
    "Usually, the threshold for IoU is kept as greater than 0.5. Although many researchers apply a much more stringent threshold like 0.6 or 0.7. If a bounding box has an IoU less than the specified threshold, that bounding box is not taken into consideration.\n",
    "\n",
    "### Non-Max Suppression (NMS):\n",
    "The algorithm may find multiple detections of the same object. Non-max suppression is a technique by which the algorithm detects the object only once. Consider an example where the algorithm detected three bounding boxes for the same object. The boxes with respective probabilities are shown in the image below.\n",
    "\n",
    "![image.png](images/nms1.png)\n",
    "\n",
    "The probabilities of the boxes are 0.7, 0.9, and 0.6 respectively. To remove the duplicates, we are first going to select the box with the highest probability and output that as a prediction. Then eliminate any bounding box with IoU > 0.5 (or any threshold value) with the predicted output. The result will be:\n",
    "\n",
    "![image.png](images/nms2.png)\n",
    "\n",
    "### Network Architecture:\n",
    "The base model has 24 convolutional layers followed by 2 fully connected layers. It uses 1 x 1 reduction layers followed by a 3 x 3 convolutional layer. Fast YOLO uses a neural network with 9 convolutional layers and fewer filters in those layers. The complete network is shown in the figure.\n",
    "\n",
    "![image.png](images/yolo_v1_architecture.png)\n",
    "\n",
    "* The architecture was designed for use in the Pascal VOC dataset, where S = 7, B = 2, and C = 20. This is the reason why final feature maps are 7 x 7, and also the output tensor is of the shape (7 x 7 x (2*5 + 20)). To use this network with a different number of classes or different grid size you might have to tune the layer dimensions.\n",
    "\n",
    "* The final layer uses a linear activation function. The rest uses a leaky ReLU.\n",
    "\n",
    "### Limitations Of YOLO:\n",
    "* Spatial constraints on bounding box predictions as each grid cell only predicts two boxes and can have only one class.\n",
    "* It is difficult to detect small objects that appear in groups.\n",
    "* It struggles to generalize objects in new or unusual aspect ratios as the model learns to predict bounding boxes from data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa7f03-e2f3-4879-86c3-d25cd49cc86a",
   "metadata": {},
   "source": [
    "## Create seperate partitions for train, test and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d10f407b-2b77-430c-b543-3610d35a04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"val\"):\n",
    "    os.makedirs(\"val\")\n",
    "    os.makedirs(os.path.join('val', 'images'))\n",
    "    os.makedirs(os.path.join('val', 'labels'))\n",
    "if not os.path.isdir(\"train\"):\n",
    "    os.makedirs(\"train\")\n",
    "    os.makedirs(os.path.join('train', 'images'))\n",
    "    os.makedirs(os.path.join('train', 'labels'))\n",
    "if not os.path.isdir(\"test\"):\n",
    "    os.makedirs(\"test\")\n",
    "    os.makedirs(os.path.join('test', 'images'))\n",
    "    os.makedirs(os.path.join('test', 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "618517c7-b8a7-4a13-9ea1-114dea3f4ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2007', 'train')\n",
      "('2007', 'val')\n",
      "('2007', 'test')\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "for set_ in sets :\n",
    "    print(set_)\n",
    "    full_file_path = os.path.join('VOCdevkit', \n",
    "                                f'VOC{set_[0]}', \n",
    "                                'ImageSets', \n",
    "                                'Main',\n",
    "                                 f'{set_[1]}.txt')\n",
    "    \n",
    "    jpeg_images_folder = os.path.join('VOCdevkit', \n",
    "                                      f'VOC{set_[0]}',\n",
    "                                     'JPEGImages')\n",
    "    \n",
    "    annotations_folder = os.path.join('VOCdevkit', \n",
    "                                      f'VOC{set_[0]}',\n",
    "                                     'Annotations')\n",
    "\n",
    "    with open(full_file_path, 'r') as f :\n",
    "        for i in f :\n",
    "            i = i.strip()\n",
    "            if i == '' : continue\n",
    "            orig_jpg_path =  os.path.join(jpeg_images_folder, f'{i}.jpg')\n",
    "            new_jpg_path = os.path.join(f'{set_[1]}', 'images', f'{i}.jpg')\n",
    "            orig_annot_path = os.path.join(annotations_folder, f'{i}.xml')\n",
    "            new_annot_path = os.path.join(f'{set_[1]}', 'labels', f'{i}.txt')\n",
    "            if not os.path.exists(new_jpg_path) :\n",
    "                shutil.copy(orig_jpg_path, new_jpg_path)\n",
    "            if not os.path.exists(new_annot_path) :\n",
    "                with open(new_annot_path, 'w') as f_w :\n",
    "                    f_w.write(convert_annotation(orig_annot_path))\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b491a5-22c9-4eb5-8f80-acddc5ccbbbe",
   "metadata": {},
   "source": [
    "## Create partition for just 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "718bc0e0-6c23-4c1b-bf0e-daf20110b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"val2\"):\n",
    "    os.makedirs(\"val2\")\n",
    "    os.makedirs(os.path.join('val2', 'images'))\n",
    "    os.makedirs(os.path.join('val2', 'labels'))\n",
    "if not os.path.isdir(\"train2\"):\n",
    "    os.makedirs(\"train2\")\n",
    "    os.makedirs(os.path.join('train2', 'images'))\n",
    "    os.makedirs(os.path.join('train2', 'labels'))\n",
    "if not os.path.isdir(\"test2\"):\n",
    "    os.makedirs(\"test2\")\n",
    "    os.makedirs(os.path.join('test2', 'images'))\n",
    "    os.makedirs(os.path.join('test2', 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f1e2c0c-28eb-4a35-b4f2-5cffad54b361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2007', 'train')\n",
      "('2007', 'val')\n",
      "('2007', 'test')\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "for set_ in sets :\n",
    "    print(set_)\n",
    "    full_file_path = os.path.join('VOCdevkit', \n",
    "                                  f'VOC{set_[0]}', \n",
    "                                  'ImageSets', \n",
    "                                  'Main',\n",
    "                                  f'person_{set_[1]}.txt')\n",
    "\n",
    "    jpeg_images_folder = os.path.join('VOCdevkit', \n",
    "                                      f'VOC{set_[0]}',\n",
    "                                      'JPEGImages')\n",
    "\n",
    "    annotations_folder = os.path.join('VOCdevkit', \n",
    "                                      f'VOC{set_[0]}',\n",
    "                                      'Annotations')\n",
    "\n",
    "    non_person = 0\n",
    "    with open(full_file_path, 'r') as f :\n",
    "        for i in f  :\n",
    "            #if non_person > 1015 : break\n",
    "            a = i.strip()\n",
    "            \n",
    "            if a == '' : continue\n",
    "            x = a.split()\n",
    "            if x[1] == '-1' :\n",
    "                continue\n",
    "                #non_person += 1\n",
    "                \n",
    "            a = x[0]\n",
    "            \n",
    "            orig_jpg_path =  os.path.join(jpeg_images_folder, f'{a}.jpg')\n",
    "            new_jpg_path = os.path.join(f'{set_[1]}2', 'images', f'{a}.jpg')\n",
    "            orig_annot_path = os.path.join(annotations_folder, f'{a}.xml')\n",
    "            new_annot_path = os.path.join(f'{set_[1]}2', 'labels', f'{a}.txt')\n",
    "            if not os.path.exists(new_jpg_path) :\n",
    "                shutil.copy(orig_jpg_path, new_jpg_path)\n",
    "            if not os.path.exists(new_annot_path) :\n",
    "                with open(new_annot_path, 'w') as f_w :\n",
    "                    f_w.write(convert_annotation(orig_annot_path))\n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c7f39e-267d-4850-86b8-e9104772e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2007', 'train')\n",
      "('2007', 'val')\n",
      "('2007', 'test')\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "for set_ in sets :\n",
    "    print(set_)\n",
    "    full_file_path = os.path.join('VOCdevkit', \n",
    "                                  f'VOC{set_[0]}', \n",
    "                                  'ImageSets', \n",
    "                                  'Main',\n",
    "                                  f'bottle_{set_[1]}.txt')\n",
    "\n",
    "    jpeg_images_folder = os.path.join('VOCdevkit', \n",
    "                                      f'VOC{set_[0]}',\n",
    "                                      'JPEGImages')\n",
    "\n",
    "    annotations_folder = os.path.join('VOCdevkit', \n",
    "                                      f'VOC{set_[0]}',\n",
    "                                      'Annotations')\n",
    "\n",
    "    non_person = 0\n",
    "    with open(full_file_path, 'r') as f :\n",
    "        for i in f  :\n",
    "            #if non_person > 1015 : break\n",
    "            a = i.strip()\n",
    "            \n",
    "            if a == '' : continue\n",
    "            x = a.split()\n",
    "            if x[1] == '-1' :\n",
    "                continue\n",
    "                #non_person += 1\n",
    "                \n",
    "            a = x[0]\n",
    "            \n",
    "            orig_jpg_path =  os.path.join(jpeg_images_folder, f'{a}.jpg')\n",
    "            new_jpg_path = os.path.join(f'{set_[1]}2', 'images', f'{a}.jpg')\n",
    "            orig_annot_path = os.path.join(annotations_folder, f'{a}.xml')\n",
    "            new_annot_path = os.path.join(f'{set_[1]}2', 'labels', f'{a}.txt')\n",
    "            if not os.path.exists(new_jpg_path) :\n",
    "                shutil.copy(orig_jpg_path, new_jpg_path)\n",
    "            if not os.path.exists(new_annot_path) :\n",
    "                with open(new_annot_path, 'w') as f_w :\n",
    "                    f_w.write(convert_annotation(orig_annot_path))\n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee4fd0-eeed-4155-99a3-ac32418b6d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
